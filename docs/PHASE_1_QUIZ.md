# AI 学习阶段性自测 (Phase 1)

这里有 6 道问题，用来测试你对目前所学内容的掌握程度。
试着回答它们，然后对照底部的参考答案。

---

## ❓ 问题

**1. LLM 的本质**
不要把 LLM 看作知识库，它的核心工作原理是什么？
A. 搜索互联网并总结答案
B. 预测下一个 Token (Next Token Prediction)
C. 存储所有维基百科的数据

**2. 关于 API 的记忆**
如果你调用 Gemini API 发送 "你好"，然后立即再次调用发送 "我刚才说了什么？"，它能回答上来吗？为什么？

**3. Tokens (词元)**
1000 个 Token 大约等于多少个英文单词？
A. 1000 个
B. 750 个
C. 500 个

**4. Embeddings (嵌入)**
在向量空间中，以下哪组词的距离（Distance）通常**最小**？
A. "猫" 和 "汽车" (拼写有点像)
B. "猫" 和 "小猫" (语义很像)
C. "猫" 和 "香蕉" (完全无关)

**5. RAG (检索增强生成)**
简述 RAG 的三个核心步骤（英文首字母 I-R-G）。

**6. Temperature (温度)**
如果你希望 AI 写一首充满想象力的诗歌，你应该把 Temperature 设置得高一点（接近 1.0）还是低一点（接近 0.0）？

---

<br/>
<br/>
<br/>
<br/>
<br/>

## ✅ 参考答案

**1. 答案：B**
LLM 本质上是一个概率统计模型，根据上文预测下一个最可能出现的词元。

**2. 答案：不能**
因为 LLM 的 API 是 **无状态 (Stateless)** 的。第二次调用时，它完全不知道第一次调用发生了什么。要让它“记住”，必须把第一次的对话历史作为 Context (上下文) 一起发给它（就像我们在 `02-chatbot.ts` 里做的那样）。

**3. 答案：B**
通常 1 Token ≈ 0.75 单词。所以 1000 Tokens ≈ 750 Words。

**4. 答案：B**
Embeddings 捕捉的是 **语义 (Meaning)**。虽然 "Cat" 和 "Car" 拼写像，但在语义空间里，"猫" 和 "小猫" 才是最接近的。

**5. 答案：**

1.  **Indexing (索引)**：把知识转成向量存起来。
2.  **Retrieval (检索)**：根据问题找最相关的知识。
3.  **Generation (生成)**：把知识给 AI，让它生成答案。

**6. 答案：高一点 (接近 1.0)**
Temperature 越高，AI 选择下一个 Token 的随机性越大，越有创造力；Temperature 越低，AI 越保守、确定，适合写代码或回答事实性问题。
