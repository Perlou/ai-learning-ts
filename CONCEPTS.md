# AI 核心概念深度解析

既然我们已经跑通了 "Hello World"，现在让我们停下来，深入理解一下底层的原理。这对于后续构建复杂的应用（如 RAG）至关重要。

## 1. LLM 的本质：概率预测机

### 它是如何“思考”的？

LLM（大语言模型）本质上是一个巨大的**统计模型**。
当你输入 `今天天气真` 时，模型并不是在“思考”天气，而是在计算：

- `好` 的概率是 70%
- `坏` 的概率是 20%
- `不错` 的概率是 9%
  ...
  然后它根据**温度 (Temperature)** 参数来通过“抽奖”的方式选择下一个字。

- **Low Temperature (0.0 - 0.3)**：输出非常确定，几乎总是选择概率最高的词。适合需要精确、可预测输出的任务（如代码生成）。
- **Medium Temperature (0.5 - 0.7)**：平衡创造性和一致性。推荐用于大多数应用。
- **High Temperature (0.8 - 1.0+)**：输出更加随机、富有创意。适合创意写作、头脑风暴。

### 幻觉 (Hallucination) 的来源

因为它是基于概率预测的，所以它可能会一本正经地胡说八道。

- 如果你问它一个它没见过的冷门知识，它会尝试“预测”最像答案的词，而不是检索事实。
- **关键点**：LLM 是**生成式**的，不是**检索式**的。

---

## 2. Embeddings (嵌入) —— AI 理解语义的关键

这是理解 RAG（检索增强生成）最核心的概念。

### 什么是 Embedding？

计算机不认识字，只认识数字。
Embedding 就是把一段文本（一个词、一句话、一篇文章）转换成**一组数字（向量）**。

例如：

- `猫` -> `[0.1, 0.5, 0.9]`
- `狗` -> `[0.1, 0.6, 0.8]`
- `苹果` -> `[0.9, 0.1, 0.2]`

### 为什么这很神奇？

在这个数学空间里，**语义相似**的词，距离会很近！

- `猫` 和 `狗` 的向量距离很近（都是宠物）。
- `猫` 和 `苹果` 的向量距离很远。

**应用**：
当我们做搜索时，不再是匹配关键词（Keyword Matching），而是匹配**语义（Semantic Matching）**。
如果你搜“好吃的红球”，传统搜索可能搜不到“苹果”；但向量搜索知道“红球”和“苹果”在语义上很近，就能搜出来。

---

## 3. Vector Database (向量数据库)

### 它是做什么的？

传统的数据库（MySQL）存的是行和列。
向量数据库（Vector DB）存的是 **Embeddings（那串数字）**。

### RAG 的工作原理 (简单版)

1. **存储**：把你的 PDF 文档切成小块，算出每一块的 Embedding，存入向量数据库。
2. **查询**：用户问“怎么请假？”，算出这句话的 Embedding。
3. **检索**：在数据库里找和“怎么请假”向量距离最近的几块文档。
4. **生成**：把找到的文档块 + 用户的问题，一起发给 LLM：“请根据这些文档回答用户的问题”。

---

## 4. Tokenization (分词) 细节

### 为什么不是按字分？

- 英文：按单词分会导致词表太大（run, running, ran...）。
- 中文：按字分通常效率较低。
- **Subword Tokenization**：这是目前的主流。它把常见的词保留，不常见的词拆开。
  - `unbelievable` -> `un` + `believ` + `able`
  - 这样既节省了词表空间，又能组合出无限的词。

### 坑点

- **算术不好**：因为数字也被切分了，LLM 对大数计算往往不擅长。
- **拼写**：如果你让 LLM 倒着拼写单词，它可能会懵，因为它看到的是 Token，不是字母。

---

## 5. Transformer 架构 (极简版)

### Attention is All You Need

Transformer 的核心机制叫 **Attention (注意力)**。
当模型在处理一句话时，它能同时“关注”到句子中相距很远的词。

例如：

> "The **animal** didn't cross the **street** because **it** was too tired."

当模型读到 `it` 时，Attention 机制告诉它，这里的 `it` 指代的是 `animal`，而不是 `street`。
这种全局的关联能力，是 Transformer 能够理解长文本、写出连贯文章的关键。

---

## 6. 提示词工程 (Prompt Engineering)

### 什么是提示词工程？

**提示词工程**是设计和优化发送给 LLM 的输入文本（提示词）的艺术和科学，目的是获得最佳输出质量。

这是使用 LLM 最重要的技能之一，因为**同一个问题，不同的提问方式会得到截然不同的结果**。

### 核心三原则：Context（上下文）、Role（角色）、Constraints（约束）

#### 1. **Context（上下文）**

提供足够的背景信息，让模型理解你的意图和场景。

**❌ 糟糕的提示词：**

```
写一篇文章
```

**✅ 优秀的提示词：**

```
为一个面向初学者的编程博客写一篇关于TypeScript基础的文章，
重点解释类型系统如何帮助减少bug。目标读者是有JavaScript经验但
从未使用过TypeScript的开发者。
```

#### 2. **Role（角色）**

告诉模型应该扮演什么角色，以获得更专业、更符合预期的输出。

**示例：**

```
你是一位资深的TypeScript架构师，拥有10年大型项目经验。
请审查以下代码并提供专业的优化建议...
```

角色设定会影响：

- 回答的专业程度
- 使用的术语
- 回答的视角和深度

#### 3. **Constraints（约束）**

明确指定输出的格式、长度、风格等限制条件。

**示例：**

```
请用3句话总结这篇文章的主要观点。
每句话不超过20个字。
使用简洁的技术术语。
```

### 进阶技巧：Chain of Thought (思维链)

**Chain of Thought (CoT)** 是一种让模型"展示推理过程"的技术，特别适合需要逻辑推理的复杂问题。

**为什么有用？**

- 提高复杂推理任务的准确率
- 让答案更可解释、可验证
- 帮助发现模型的错误思路

**如何使用：**
在提示词中加入"让我们一步步思考"或"请展示你的推理过程"。

**示例：**

```
问题：小明有15个苹果，他给了小红1/3，然后自己又吃了2个。
剩下的苹果平均分给3个朋友，每人能得到几个？

请一步步展示你的计算过程。
```

### 结构化输出（JSON Mode）

强制 LLM 输出特定格式的数据（如 JSON），便于程序解析和处理。

**为什么需要？**

- 提高输出的可靠性和一致性
- 方便代码集成
- 减少解析错误

**示例：**

```
从以下文本中提取人名、地点和日期，以JSON格式输出：

"张三将于2024年3月15日在北京参加技术大会。"

要求输出格式：
{
  "person": "...",
  "location": "...",
```

"date": "..."
}

````

**提示词工程的黄金法则：**
1. **明确具体**：不要让模型猜测你想要什么
2. **提供示例**：Few-shot learning（给几个例子）比纯描述更有效
3. **迭代优化**：第一版提示词通常不是最好的，需要不断测试和改进
4. **测试边界情况**：用各种输入测试你的提示词是否稳健

---

## 8. 本地 LLM 部署

### 8.1 云端 API vs 本地部署

**对比分析：**

| 维度 | 云端API (Gemini/GPT-4) | 本地LLM (Ollama) |
|------|------------------------|------------------|
| **成本** | 按使用付费 💰 | 完全免费 ✅ |
| **隐私** | 数据发送到云端 | 数据不离开本地 🔒 |
| **延迟** | 网络往返（100-500ms） | 本地推理（快） ⚡ |
| **模型能力** | 最强（GPT-4, Gemini Pro） | 较弱但足够用 |
| **离线使用** | 需要网络 | 完全离线 🌐 |
| **硬件要求** | 无 | 需要足够RAM/GPU |
| **维护** | 无需维护 | 需要管理模型 |

**何时使用本地LLM：**
- ✅ 处理敏感数据（医疗、法律、金融）
- ✅ 高频调用（成本控制）
- ✅ 离线环境
- ✅ 学习AI原理
- ✅ 开发和测试

**何时使用云端API：**
- ✅ 需要最强能力
- ✅ 低频使用
- ✅ 无硬件限制
- ✅ 快速原型开发

---

### 8.2 Ollama 简介

**什么是 Ollama？**

Ollama 是一个让你在本地运行大语言模型的工具，就像在本地运行 Docker 一样简单。

```bash
# 安装 Ollama
brew install ollama

# 下载模型
ollama pull qwen2.5:7b

# 运行模型
ollama run qwen2.5:7b
````

**核心特性：**

- 🚀 一键下载和运行模型
- 🔌 RESTful API（兼容 OpenAI 格式）
- 💻 支持 macOS, Linux, Windows
- ⚡ 自动 GPU 加速（Metal/CUDA）
- 📦 模型管理（类似 Docker）

---

### 8.3 模型量化 (Quantization)

**为什么需要量化？**

原始模型太大，无法在普通电脑运行：

- Llama 3 70B（原始）: ~140GB
- Llama 3 70B（4-bit 量化）: ~38GB

**量化技术：**

```
原始精度 (FP32)       → 32位浮点数 (100%)
半精度 (FP16)         → 16位浮点数 (50%)
8-bit 量化            → 8位整数 (25%)
4-bit 量化 (GPTQ/GGUF) → 4位 (12.5%)
```

**性能 vs 精度权衡：**

- **4-bit**: 轻微质量损失，75%内存节省
- **8-bit**: 几乎无质量损失，50%内存节省
- **FP16**: 无质量损失，但仍需大内存

**Ollama 默认使用 4-bit 量化（GGUF 格式）**

---

### 8.4 推荐模型

**中文优化模型：**

| 模型          | 参数 | 大小  | RAM 需求 | 特点            |
| ------------- | ---- | ----- | -------- | --------------- |
| `qwen2.5:7b`  | 7B   | 4.7GB | 8GB      | 阿里，中文强 🌟 |
| `glm4:9b`     | 9B   | 5.5GB | 8GB      | 清华，对话好    |
| `qwen2.5:14b` | 14B  | 9GB   | 16GB     | 更强大          |

**英文模型：**

| 模型          | 参数 | 大小  | RAM 需求 | 特点         |
| ------------- | ---- | ----- | -------- | ------------ |
| `llama3.1:8b` | 8B   | 4.7GB | 8GB      | Meta 官方 🌟 |
| `mistral:7b`  | 7B   | 4.1GB | 8GB      | 高性能       |
| `gemma2:9b`   | 9B   | 5.5GB | 8GB      | Google       |

**垂直领域：**

- 代码：`codellama:7b`, `deepseek-coder:6.7b`
- 数学：`qwen-math:7b`
- 快速测试：`gemma2:2b`, `phi3:mini`

---

### 8.5 Ollama API

**主要端点：**

```typescript
// 1. 文本生成
POST http://localhost:11434/api/generate
{
  "model": "qwen2.5:7b",
  "prompt": "解释量子计算",
  "stream": false
}

// 2. 对话模式
POST http://localhost:11434/api/chat
{
  "model": "qwen2.5:7b",
  "messages": [
    { "role": "user", "content": "你好" }
  ]
}

// 3. 生成Embeddings
POST http://localhost:11434/api/embeddings
{
  "model": "qwen2.5:7b",
  "prompt": "向量化这段文本"
}
```

**流式响应：**

```typescript
const response = await fetch("http://localhost:11434/api/generate", {
  method: "POST",
  body: JSON.stringify({
    model: "qwen2.5:7b",
    prompt: "写一首诗",
    stream: true, // 启用流式
  }),
});

// 逐块读取
for await (const chunk of response.body) {
  const data = JSON.parse(chunk);
  process.stdout.write(data.response);
}
```

---

### 8.6 性能优化

**硬件加速：**

```bash
# macOS（Apple Silicon）
# 自动使用Metal加速，无需配置

# Linux（NVIDIA GPU）
# 自动检测CUDA，确保驱动已安装

# 检查GPU使用
nvidia-smi  # Linux
```

**内存管理：**

```bash
# 限制上下文长度减少内存
ollama run qwen2.5:7b --ctx-size 2048

# 卸载不用的模型
ollama rm old-model
```

**并发控制：**

```bash
# 设置并发请求数
export OLLAMA_NUM_PARALLEL=2
ollama serve
```

---

### 8.7 本地 vs 云端：实战建议

**混合使用策略：**

```typescript
async function chooseModel(task: string, sensitive: boolean) {
  if (sensitive) {
    // 敏感数据用本地
    return await callOllama(task);
  }

  if (task.complexity === "high") {
    // 复杂任务用云端
    return await callGemini(task);
  }

  // 一般任务用本地（省钱）
  return await callOllama(task);
}
```

**成本分析示例：**

```
场景：每天1000次调用

云端方案（Gemini）:
- 1000 calls × $0.005 = $5/day
- 月成本: $150

本地方案（Ollama + Qwen2.5）:
- API成本: $0
- 硬件成本: 一次性（Mac已有）
- 月成本: $0

3个月后本地方案回本！
```

---

### 8.8 局限性

**本地 LLM 的限制：**

1. **能力天花板**: 7B-14B 模型 < GPT-4/Gemini Pro
2. **硬件依赖**: 需要足够 RAM（最少 8GB）
3. **维护成本**: 需要管理模型、更新
4. **功能支持**:
   - ❌ 原生 Function Calling（需手动实现）
   - ❌ 图像生成
   - ❌ 多模态（部分模型支持）

**适用场景总结：**

```
本地LLM适合：
✅ 数据敏感场景
✅ 高频简单任务
✅ 离线环境
✅ 学习和实验

云端API适合：
✅ 复杂推理任务
✅ 低频使用
✅ 需要最新能力
✅ 生产关键应用
```

---

**关键要点：**

- 本地 LLM 是云端 API 的补充，不是替代
- 根据场景选择合适的方案
- 可以混合使用，发挥各自优势
- Ollama 让本地部署变得非常简单

继续下一章了解如何在代码中使用 Ollama！

---

## 7. Agents（智能体）与 Function Calling

### 什么是 AI Agent？

**传统 LLM（聊天机器人）：**

```

用户: "北京今天天气怎么样？"
AI: "抱歉，我无法获取实时天气信息..."

```

**AI Agent（智能体）：**

```

用户: "北京今天天气怎么样？"
AI: [自动调用天气 API]
AI: "北京今天晴天，温度 15-25 度，空气质量良好。"

```

**核心区别：**

- 传统 LLM 只能**对话**
- Agent 可以**行动**（调用工具、访问数据、执行操作）

### Function Calling 的工作原理

**Function Calling**（函数调用/工具调用）是让 AI 使用外部工具的技术。

**完整流程：**

```

1. 定义工具
   ↓
2. 用户提问
   ↓
3. AI 判断是否需要工具
   ↓
4. 如果需要，AI 生成函数调用请求（含参数）
   ↓
5. 程序执行实际的函数
   ↓
6. 将结果返回给 AI
   ↓
7. AI 基于结果生成最终回答

```

**示例对话流程：**

**步骤 1：定义工具**

```typescript
const tools = [
  {
    name: "get_weather",
    description: "获取城市天气信息",
    parameters: {
      city: { type: "string", description: "城市名" },
    },
  },
];
```

**步骤 2-3：用户提问，AI 判断**

```
用户: "北京今天天气怎么样？"
AI思考: 这需要实时天气数据，我应该调用 get_weather 工具
```

**步骤 4：AI 生成函数调用**

```json
{
  "name": "get_weather",
  "arguments": {
    "city": "北京"
  }
}
```

**步骤 5：程序执行函数**

```typescript
const result = getWeather("北京");
// 返回: { temp: 20, condition: "晴天" }
```

**步骤 6-7：AI 生成最终回答**

```
AI: "北京今天天气晴朗，温度约20度。"
```

### Function Calling vs 传统编程

| 传统编程            | Function Calling        |
| ------------------- | ----------------------- |
| 你决定调用哪个函数  | **AI 决定**调用哪个函数 |
| 你写死参数          | AI 从对话中**提取参数** |
| 固定的 if-else 逻辑 | AI**理解意图**并规划    |
| 无法处理模糊输入    | AI 可以**理解自然语言** |

**示例对比：**

**传统方式：**

```typescript
if (input.includes("天气") && input.includes("北京")) {
  getWeather("北京");
} else if (input.includes("天气") && input.includes("上海")) {
  getWeather("上海");
}
// 需要处理所有可能的组合...
```

**Function Calling 方式：**

```typescript
// AI自动理解并调用
"北京天气" → get_weather("北京")
"帝都今天冷吗" → get_weather("北京")
"BJ的温度" → get_weather("北京")
```

### 工具定义格式（JSON Schema）

工具定义遵循 **JSON Schema** 规范：

```typescript
{
  name: 'function_name',           // 函数名（必须）
  description: '函数功能描述',      // 告诉AI什么时候用（必须）
  parameters: {                     // 参数定义
    type: 'object',
    properties: {
      param1: {
        type: 'string',             // 类型
        description: '参数说明'      // 帮助AI理解
      },
      param2: {
        type: 'number',
        enum: [1, 2, 3]            // 可选值限制
      }
    },
    required: ['param1']           // 必需参数
  }
}
```

**重要原则：**

1. **description 非常重要**：AI 通过描述决定何时使用工具
2. **参数描述要清晰**：帮助 AI 正确提取参数
3. **使用枚举**：限制参数值，提高准确率

### 实际应用场景

**1. 数据查询**

```
用户: "显示上个月的销售额"
AI: [调用数据库查询函数] "上个月销售额为120万元"
```

**2. 外部 API 集成**

```
用户: "帮我查一下AAPL的股价"
AI: [调用股票API] "苹果公司当前股价为180.5美元"
```

**3. 计算任务**

```
用户: "如果我买了100股，每股150元，总共多少钱？"
AI: [调用计算器] "总共15000元"
```

**4. 自动化操作**

```
用户: "给张三发个邮件，主题是会议提醒"
AI: [调用邮件API] "已发送邮件给张三"
```

### 多工具协作

Agent 可以同时使用多个工具来完成复杂任务：

**示例：旅行规划**

```
用户: "我下周要去上海出差，帮我安排一下"

AI思考过程:
1. 调用天气API → 获取上海下周天气
2. 调用航班API → 查询机票价格
3. 调用酒店API → 推荐酒店
4. 综合信息给出建议
```

### 常见模型的 Function Calling 支持

| 模型             | 支持情况            | API 关键字             |
| ---------------- | ------------------- | ---------------------- |
| **Gemini 2.0**   | ✅ 完全支持         | `functionDeclarations` |
| **GPT-4/GPT-4o** | ✅ 完全支持         | `tools`                |
| **Claude 3.5**   | ✅ 支持（Tool Use） | `tools`                |
| **Llama 3.1+**   | ✅ 支持             | `tools`                |

### 限制与注意事项

**1. AI 不保证 100%正确**

- 可能选错工具
- 可能提取错参数
- 需要验证和容错处理

**2. 成本考虑**

- 多次调用会消耗更多 tokens
- 需要平衡准确率和成本

**3. 安全性**

- 不要暴露敏感工具（删除数据、发送邮件等）
- 添加权限验证
- 对工具调用进行日志记录

**4. 性能**

- 每次工具调用都是一次 API 往返
- 复杂任务可能需要多轮对话

### 从对话到行动

**这是 AI 应用的范式转变：**

**过去：** AI = 信息提供者

```
"告诉我天气" → AI提供建议
```

**现在：** AI = 行动执行者

```
"帮我安排明天的日程" → AI调用日历API实际创建事件
```

**未来：** AI = 自主代理

```
"优化我的投资组合" → AI分析数据、执行交易、监控结果
```

**Function Calling 是通往真正智能助手的关键技术！**

```

```
