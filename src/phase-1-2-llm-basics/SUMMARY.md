# 第 1-2 阶段学习总结：LLM 基础与提示词工程

> **本阶段目标**：理解大语言模型的工作原理，掌握与 LLM 的交互方式，学会高效的提示词工程技巧

---

## 📚 核心知识点

### 一、LLM（大语言模型）基础

#### 1.1 LLM 的本质

**关键理解**：LLM 不是知识库，而是**下一个词元预测引擎**。

```
输入："北京的天气"
模型思考："今天"、"怎么样"、"如何" 的概率最高
输出："北京的天气今天怎么样？"
```

**核心特性**：

- ✅ **概率生成**：基于训练数据的统计规律生成文本
- ❌ **幻觉（Hallucination）**：可能自信地编造不存在的事实
- 🔄 **无状态（Stateless）**：API 不会记住之前的对话，需要显式传递上下文

#### 1.2 Tokens（词元）

LLM 处理的最小单位不是字符或单词，而是 Token。

**Token 规则**：

- 英文：`1 Token ≈ 0.75 个单词`
- 中文：`1 Token ≈ 0.5-1 个字`
- 常见词：`"the"` = 1 token
- 罕见词：`"TypeScript"` 可能 = 2-3 tokens

**为什么重要？**

- API 按 Token 计费
- Context Window 限制（如 Gemini 1.5 Pro 支持 1M tokens）
- 影响响应速度

#### 1.3 Context（上下文）管理

LLM 的"记忆"来自于你在每次请求中发送的完整对话历史。

**对话示例**：

```typescript
// ❌ 错误：只发送当前消息
await model.generateContent("那明天呢？");
// AI 不知道"那"指什么

// ✅ 正确：发送完整上下文
const chat = model.startChat({
  history: [
    { role: "user", parts: [{ text: "北京今天天气怎么样？" }] },
    { role: "model", parts: [{ text: "今天晴天，15度。" }] },
  ],
});
await chat.sendMessage("那明天呢？");
// AI 知道你在问北京明天的天气
```

---

### 二、提示词工程（Prompt Engineering）

> **核心洞察**：同样的模型，不同的提问方式，输出质量可以相差 10 倍甚至 100 倍。

#### 2.1 三大核心原则

提示词工程的黄金法则：**Context + Role + Constraints**

##### 原则一：Context（上下文）

提供足够的背景信息，让模型理解你的意图。

**对比示例**：

❌ **糟糕（无上下文）**：

```
"写一篇文章"
```

→ AI 会反问：什么主题？什么风格？给谁看？

✅ **优秀（有上下文）**：

```
为一个面向初学者的编程博客写一篇关于 TypeScript 基础的文章，
重点解释类型系统如何帮助减少 bug。目标读者是有 JavaScript
经验但从未使用过 TypeScript 的开发者。
```

→ AI 直接生成符合要求的内容！

##### 原则二：Role（角色）

告诉模型应该扮演什么角色，激活相应的专业知识。

**常用角色**：

- `"你是一位资深的 TypeScript 架构师"`
- `"你是一位经验丰富的技术导师"`
- `"你是一位擅长代码审查的工程师"`

**角色的影响**：
| 角色 | 输出特点 |
|------|----------|
| 资深架构师 | 使用专业术语、关注架构设计、考虑可扩展性 |
| 技术导师 | 解释清晰、循序渐进、注重基础概念 |
| 代码审查者 | 关注代码质量、性能优化、最佳实践 |

##### 原则三：Constraints（约束）

明确指定输出的格式、长度、风格、限制条件。

**约束类型**：

**1. 格式约束**

```
- 使用 Markdown 格式
- 以 JSON 格式输出
- 分 3 段，每段不超过 100 字
```

**2. 长度约束**

```
- 用 3 句话总结
- 不超过 300 字
- 列出前 5 个要点
```

**3. 风格约束**

```
- 使用简洁的技术术语
- 友好、鼓励性的语气
- 正式的商务报告风格
```

**4. 内容约束**

```
- 只关注性能优化方面
- 不要包含实现细节
- 避免使用专业术语
```

#### 2.2 高级技巧：Chain of Thought（CoT）

**什么是 CoT？**

让 AI 像人类一样"想出声"，展示从问题到答案的每一步推理。

**触发关键词**：

- `"让我们一步步思考"`
- `"请展示你的推理过程"`
- `"请一步步计算"`

**示例对比**：

❌ **不使用 CoT**：

```
问题：小明有15个苹果，给了小红1/3，然后自己又吃了2个。
剩下的平均分给3个朋友，每人得几个？

答案：每人 2 个。（看不到推理过程）
```

✅ **使用 CoT**：

```
请一步步展示计算过程。

答案：
1. 小明给小红：15 × 1/3 = 5 个
2. 小明剩余：15 - 5 = 10 个
3. 吃掉 2 个后：10 - 2 = 8 个
4. 分给 3 个朋友：8 ÷ 3 ≈ 2.67 个

最终答案：每人可以得到 2 个完整的苹果。
```

**CoT 的价值**：

- ✅ 提高准确率
- ✅ 可验证性（能看到每一步）
- ✅ 可调试性（知道哪一步出错）
- ✅ 可解释性（理解 AI 的推理）

**最适合的场景**：

- 数学计算题
- 逻辑推理题
- 复杂决策分析
- 多步骤问题求解

#### 2.3 结构化输出：JSON Mode

**为什么需要？**

自然语言输出难以解析：

```
AI: 这个人叫张三，住在北京，今年30岁...
```

→ 需要复杂的正则表达式或文本解析器

结构化 JSON 输出：

```json
{
  "name": "张三",
  "location": "北京",
  "age": 30
}
```

→ 直接用 `JSON.parse()` 解析！

**如何实现？**

**关键要素**：

1. 明确要求"纯 JSON，不要任何额外说明"
2. 提供具体的 JSON 结构模板
3. 使用 TypeScript 接口定义预期结构

**示例提示词**：

```
从以下文本中提取关键信息，并以 JSON 格式输出：

文本：张三将于2024年3月15日在北京参加技术大会。

要求输出格式（纯JSON，不要任何额外说明）：
{
  "person": "人名",
  "location": "地点",
  "date": "日期",
  "event": "事件"
}
```

**TypeScript 集成**：

```typescript
interface ExtractedData {
  person: string;
  location: string;
  date: string;
  event: string;
}

const data: ExtractedData = JSON.parse(response);
console.log(data.person); // "张三"
```

**应用场景**：

- 信息提取（NER - 命名实体识别）
- 数据格式转换
- API 响应生成
- 批量数据处理

---

## 💻 代码示例总结

### 示例 1：Hello Gemini（01-hello-gemini.ts）

**学习要点**：

- ✅ Gemini API 基础调用
- ✅ 环境变量配置（dotenv）
- ✅ 异步操作（async/await）
- ✅ 错误处理

**核心代码**：

```typescript
const genAI = new GoogleGenerativeAI(apiKey);
const model = genAI.getGenerativeModel({ model: "gemini-2.5-flash" });

const result = await model.generateContent(prompt);
const text = result.response.text();
```

---

### 示例 2：聊天机器人（02-chatbot.ts）

**学习要点**：

- ✅ 对话历史管理（startChat）
- ✅ Context Window 概念
- ✅ 多轮对话实现
- ✅ readline 终端交互

**核心代码**：

```typescript
const chat = model.startChat({
  history: [
    { role: "user", parts: [{ text: "你好" }] },
    { role: "model", parts: [{ text: "你好！" }] },
  ],
});

// SDK 自动管理历史
await chat.sendMessage("我之前说了什么？");
```

**关键理解**：

- `startChat` 会自动维护对话历史
- 每次调用 `sendMessage` 都会更新历史
- 旧对话会占用 Context Window

---

### 示例 3：提示词基础（05-prompt-basics.ts）

**学习要点**：

- ✅ 对比糟糕 vs 优秀的提示词
- ✅ Context + Role + Constraints 三原则
- ✅ 提示词模板设计

**对比实验**：

```typescript
// 糟糕：模糊、缺乏上下文
const badPrompt = "写一篇文章";

// 优秀：包含完整的上下文、角色和约束
const goodPrompt = `你是一位资深的技术写作专家。

请为一个面向初学者的编程博客写一篇关于TypeScript基础的文章。

目标读者：有JavaScript经验但从未使用过TypeScript的开发者
重点内容：解释类型系统如何帮助减少bug
文章要求：
- 长度：300字左右
- 语言：简洁易懂
- 结构：包含引言、核心观点、实际示例、总结`;
```

---

### 示例 4：思维链推理（06-chain-of-thought.ts）

**学习要点**：

- ✅ Chain of Thought（CoT）技术
- ✅ 复杂问题分步求解
- ✅ 提高推理准确性

**核心模式**：

```typescript
const problem = `小明有15个苹果，他给了小红其中的1/3，
然后自己又吃了2个。剩下的苹果平均分给3个朋友，
每人能得到几个？`;

// 触发 CoT
const cotPrompt = `${problem}

请一步步展示你的计算过程，列出每一步的算式和结果。
最后给出最终答案。`;
```

**CoT 关键词**：

- "一步步思考"
- "展示推理过程"
- "列出计算步骤"

---

### 示例 5：结构化输出（07-structured-output.ts）

**学习要点**：

- ✅ 强制 JSON 输出
- ✅ TypeScript 类型定义
- ✅ 数据解析与验证

**核心模式**：

```typescript
interface ExtractedData {
  person: string;
  location: string;
  date: string;
  event: string;
}

const prompt = `从以下文本中提取关键信息，并以JSON格式输出：

文本：${text}

要求输出格式（纯JSON，不要任何额外说明）：
{
  "person": "人名",
  "location": "地点",
  "date": "日期",
  "event": "事件"
}`;

const data: ExtractedData = JSON.parse(response);
```

**注意事项**：

- ⚠️ 强调"纯 JSON，不要任何额外说明"
- ⚠️ 提供明确的 JSON 结构示例
- ⚠️ 使用 try-catch 处理解析错误

---

## 🎯 提示词质量自检清单

在发送提示词前，问自己这些问题：

- [ ] **上下文充足**：AI 能理解我的场景和目标吗？
- [ ] **角色明确**：我指定了合适的角色吗？
- [ ] **约束清晰**：输出格式、长度、风格都明确了吗？
- [ ] **示例充分**：需要示例的话，我提供了吗？
- [ ] **可测试性**：我能验证输出是否正确吗？
- [ ] **边界考虑**：我考虑过异常情况了吗？

---

## 🚀 实战技巧公式

### 场景 1：写作任务

```
角色设定 + 目标受众 + 内容主题 + 写作风格 + 长度限制
```

### 场景 2：代码生成

```
技术角色 + 编程语言 + 功能描述 + 代码要求 + 注释要求
```

### 场景 3：数据分析

```
分析角色 + 数据描述 + 分析目标 + 输出格式
```

---

## 🔄 常见问题与解决方案

### Q1: AI 的回答太长/太短

**解决**：在约束中明确长度

```
- 用 3 句话总结
- 控制在 300 字以内
- 列出前 5 个要点
```

### Q2: AI 的回答不够专业

**解决**：设定专业角色

```
你是一位拥有 10 年经验的资深 XX 专家...
```

### Q3: AI 理解错了我的意图

**解决**：提供更多上下文

```
背景：我是一个初学者
目标：学习 TypeScript 基础
问题：...
```

### Q4: JSON 输出包含额外文字

**解决**：强调"纯 JSON"

```
要求输出格式（纯 JSON，不要任何额外说明）：
{...}
```

---

## ✅ 本阶段核心要点

1. **LLM 本质**：下一个词元预测引擎，基于概率生成文本
2. **Tokens**：LLM 的最小处理单位，影响成本和性能
3. **Context**：LLM 的"记忆"需要显式管理
4. **三原则**：Context + Role + Constraints 是提示词基石
5. **CoT**：复杂任务必备，展示推理过程提高准确性
6. **结构化输出**：JSON 格式方便程序处理

---

## 📈 下一步学习建议

1. **实践为王**：尝试编写不同场景的提示词
2. **建立模板库**：保存好用的提示词模板
3. **持续迭代**：不断测试和优化提示词
4. **深入理解**：完成测验题检验学习成果

**恭喜你完成第 1-2 阶段的学习！** 🎉

你已经掌握了 LLM 的核心概念和提示词工程技巧，这是使用 LLM 最基础也是最关键的能力。

👉 **下一步**：进入[第 3 阶段](../phase-3-embeddings-rag/README.md)学习 Embeddings 与 RAG！
